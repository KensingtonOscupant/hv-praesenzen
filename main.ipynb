{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfelixringe\u001b[0m (\u001b[33mfuels\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/gisbertgurke/Documents/Projekte/hv-praesenzen/wandb/run-20231222_121422-w5a6j7qv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fuels/hv-praesenzen/runs/w5a6j7qv' target=\"_blank\">fast-silence-123</a></strong> to <a href='https://wandb.ai/fuels/hv-praesenzen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fuels/hv-praesenzen' target=\"_blank\">https://wandb.ai/fuels/hv-praesenzen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fuels/hv-praesenzen/runs/w5a6j7qv' target=\"_blank\">https://wandb.ai/fuels/hv-praesenzen/runs/w5a6j7qv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 234/1000 [17:00<55:39,  4.36s/it]  \n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-1106-preview in organization org-7Ue91zd9n9SDHlHsN2DVWPxD on tokens_usage_based per day: Limit 5000000, Used 4998554, Requested 1603. Please try again in 2.712s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens_usage_based', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/gisbertgurke/Documents/Projekte/hv-praesenzen/main.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gisbertgurke/Documents/Projekte/hv-praesenzen/main.ipynb#W0sZmlsZQ%3D%3D?line=192'>193</a>\u001b[0m \u001b[39mif\u001b[39;00m column_names \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m[0]\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gisbertgurke/Documents/Projekte/hv-praesenzen/main.ipynb#W0sZmlsZQ%3D%3D?line=194'>195</a>\u001b[0m     user_prompt \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mprompts\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39muser_prompts\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39malternative_doc_structure_prompt\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mformat(report\u001b[39m=\u001b[39mfull_text)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/gisbertgurke/Documents/Projekte/hv-praesenzen/main.ipynb#W0sZmlsZQ%3D%3D?line=196'>197</a>\u001b[0m     response, cost \u001b[39m=\u001b[39m call_language_model(client, system_prompt, user_prompt, full_text, calculate_cost)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gisbertgurke/Documents/Projekte/hv-praesenzen/main.ipynb#W0sZmlsZQ%3D%3D?line=197'>198</a>\u001b[0m     header_row_list\u001b[39m.\u001b[39mappend(response)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gisbertgurke/Documents/Projekte/hv-praesenzen/main.ipynb#W0sZmlsZQ%3D%3D?line=198'>199</a>\u001b[0m     data[\u001b[39m'\u001b[39m\u001b[39mcost\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m cost\n",
      "File \u001b[0;32m~/Documents/Projekte/hv-praesenzen/utilities.py:91\u001b[0m, in \u001b[0;36mcall_language_model\u001b[0;34m(client, system_prompt, user_prompt, full_text, calculate_cost)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39mMakes a call to the GPT language model and returns the response and the cost.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mtuple: A tuple containing the response from GPT and the cost of the call.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m combined_prompt \u001b[39m=\u001b[39m user_prompt \u001b[39m+\u001b[39m full_text\n\u001b[0;32m---> 91\u001b[0m response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     92\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-4-1106-preview\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     93\u001b[0m     messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     94\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: system_prompt},\n\u001b[1;32m     95\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: combined_prompt}\n\u001b[1;32m     96\u001b[0m     ],\n\u001b[1;32m     97\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m input_tokens \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39musage\u001b[39m.\u001b[39mprompt_tokens\n\u001b[1;32m    101\u001b[0m output_tokens \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39musage\u001b[39m.\u001b[39mcompletion_tokens\n",
      "File \u001b[0;32m~/Documents/Projekte/hv-praesenzen/hv_venv/lib/python3.10/site-packages/openai/_utils/_utils.py:303\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 303\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Projekte/hv-praesenzen/hv_venv/lib/python3.10/site-packages/openai/resources/chat/completions.py:604\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    558\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    559\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    602\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    603\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 604\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    605\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    606\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    607\u001b[0m             {\n\u001b[1;32m    608\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    609\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    615\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    616\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    617\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    618\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    619\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    620\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    621\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    622\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    623\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    624\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    625\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    626\u001b[0m             },\n\u001b[1;32m    627\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    628\u001b[0m         ),\n\u001b[1;32m    629\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    630\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    631\u001b[0m         ),\n\u001b[1;32m    632\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    633\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    634\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    635\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projekte/hv-praesenzen/hv_venv/lib/python3.10/site-packages/openai/_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1075\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1076\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1084\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1085\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1086\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1087\u001b[0m     )\n\u001b[0;32m-> 1088\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/Documents/Projekte/hv-praesenzen/hv_venv/lib/python3.10/site-packages/openai/_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    845\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    851\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    852\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 853\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    854\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    855\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    856\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    857\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    858\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    859\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projekte/hv-praesenzen/hv_venv/lib/python3.10/site-packages/openai/_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m    915\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 916\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m    917\u001b[0m         options,\n\u001b[1;32m    918\u001b[0m         cast_to,\n\u001b[1;32m    919\u001b[0m         retries,\n\u001b[1;32m    920\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    921\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    922\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    925\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Documents/Projekte/hv-praesenzen/hv_venv/lib/python3.10/site-packages/openai/_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    956\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 958\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    959\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    960\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    961\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m    962\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    963\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    964\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Projekte/hv-praesenzen/hv_venv/lib/python3.10/site-packages/openai/_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m    915\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 916\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m    917\u001b[0m         options,\n\u001b[1;32m    918\u001b[0m         cast_to,\n\u001b[1;32m    919\u001b[0m         retries,\n\u001b[1;32m    920\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    921\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    922\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    925\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Documents/Projekte/hv-praesenzen/hv_venv/lib/python3.10/site-packages/openai/_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    956\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 958\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    959\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    960\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    961\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m    962\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    963\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    964\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Projekte/hv-praesenzen/hv_venv/lib/python3.10/site-packages/openai/_base_client.py:930\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n\u001b[1;32m    928\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 930\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m    933\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m    934\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    937\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m    938\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-1106-preview in organization org-7Ue91zd9n9SDHlHsN2DVWPxD on tokens_usage_based per day: Limit 5000000, Used 4998554, Requested 1603. Please try again in 2.712s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens_usage_based', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import ast\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import wandb\n",
    "import json\n",
    "from config import api_key\n",
    "from utilities import calculate_cost, find_subdirectory, find_matching_files, process_pdf, call_language_model\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Load the config file\n",
    "def load_config(config_file):\n",
    "    with open(config_file, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "config = load_config('config.json')\n",
    "\n",
    "# Load the CSV file (test data or full data)\n",
    "df = pd.read_csv(config['csv_file_path'])\n",
    "# Set the base directory for where the folders with the PDFs are located\n",
    "directory = config['directory']\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "\n",
    "run = wandb.init(project=config['wandb_project_name'])\n",
    "# Define W&B Table to store results\n",
    "columns = config['wandb_table_columns_prod']\n",
    "table = wandb.Table(columns=columns)\n",
    "# log a row of data to the W&B table\n",
    "def log_to_wandb(data):\n",
    "    table.add_data(*data.values())\n",
    "\n",
    "# Log the config.json file as an artifact\n",
    "artifact = wandb.Artifact('run_configurations', type='config')\n",
    "artifact.add_file('config.json')\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "# Assuming args.start and args.end are defined (the row numbers to start and end with)\n",
    "# start_index = args.start if args.start is not None else 0\n",
    "# end_index = args.end if args.end is not None else len(df)\n",
    "\n",
    "start_index = 1230\n",
    "end_index = 2000\n",
    "\n",
    "# Calculate the total number of rows to be processed\n",
    "total_rows = end_index - start_index\n",
    "\n",
    "# Main application logic\n",
    "if __name__ == \"__main__\":\n",
    "    # # Use args and api_key as needed\n",
    "    # print(f\"Start: {args.start}, End: {args.end}\")\n",
    "\n",
    "    # keep track of processed files\n",
    "    processed_files = {}\n",
    "\n",
    "    # Iterate over the DataFrame slice with tqdm\n",
    "    for index, row in tqdm(df.iloc[start_index:end_index].iterrows(), total=total_rows):\n",
    "\n",
    "        # if Presence is already present, skip this row\n",
    "        if pd.notna(row['Presence']):\n",
    "            continue\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize the default data dictionary for each iteration. \n",
    "        This will be updated with the results of each step and then logged to W&B.\n",
    "        \"\"\"\n",
    "        data = {\n",
    "            \"unique_id\": str(row['ID']),\n",
    "            \"ID_Key\": str(int(row['ID_Key'])),\n",
    "            \"Year\": str(int(row['Year'])),\n",
    "            \"Presence\": None,\n",
    "            \"error\": None,\n",
    "            \"cost\": 0,\n",
    "            \"header_row\": None,\n",
    "            \"all_percentage_values\": None,\n",
    "            \"file_path\": None,\n",
    "            \"comment\": \"\"\n",
    "        }\n",
    "\n",
    "        # set flag for alternative document structure, i.e., no table but more like a list\n",
    "        alternative_document_structure = False\n",
    "\n",
    "        id_value = str(int(row['ID_Key']))\n",
    "        year_suffix = str(int(row['Year']))[-2:]\n",
    "\n",
    "        # find the subdirectory where all reports for the same ID are located\n",
    "        subdirectory_path = find_subdirectory(directory, id_value)\n",
    "\n",
    "        # find the file path for the report(s) of the given year\n",
    "        matching_files = find_matching_files(subdirectory_path, year_suffix)\n",
    "\n",
    "        # Initialize the key in the dictionary if it doesn't exist\n",
    "        key = (id_value, year_suffix)\n",
    "        if key not in processed_files:\n",
    "            processed_files[key] = []\n",
    "\n",
    "        # Find the first unprocessed file\n",
    "        unprocessed_files = [file for file in matching_files if file not in processed_files[key]]\n",
    "        if not unprocessed_files:\n",
    "            # Log the scenario where the record exists but no file is found\n",
    "            data.update({\n",
    "                \"error\": \"File missing\"\n",
    "            })\n",
    "            log_to_wandb(data)\n",
    "            continue\n",
    "\n",
    "        # Process the first unprocessed file\n",
    "        file_path = unprocessed_files[0]\n",
    "\n",
    "        # mark the file as processed\n",
    "        processed_files[key].append(file_path)\n",
    "\n",
    "        if file_path:\n",
    "            data[\"file_path\"] = file_path\n",
    "            full_text, error_message = process_pdf(file_path)\n",
    "\n",
    "            # Check if there was an error processing the PDF\n",
    "            if error_message:\n",
    "                data.update({\n",
    "                    \"error\": error_message,\n",
    "                    \"comment\": \"Error processing PDF document\"\n",
    "                })\n",
    "                log_to_wandb(data)\n",
    "                continue\n",
    "\n",
    "        # In case of file not found\n",
    "        else:\n",
    "            data.update({\n",
    "                \"error\": \"File not found\",\n",
    "                \"comment\": f\"No file found for ID {id_value}\"\n",
    "            })\n",
    "            log_to_wandb(data)\n",
    "            continue\n",
    "\n",
    "        # add an error if text is more than 15000 characters\n",
    "        if len(full_text) > 15000:\n",
    "            data.update({\n",
    "                \"error\": \"Text longer than 15000 characters\",\n",
    "            })\n",
    "            log_to_wandb(data)\n",
    "            continue\n",
    "\n",
    "        highest_percentage_list = []\n",
    "\n",
    "        system_prompt = config['prompts']['system_prompt']\n",
    "        user_prompt = config['prompts']['user_prompts']['header_evaluation_prompt'].format(report=full_text)\n",
    "\n",
    "        header_row_list = []\n",
    "\n",
    "        for i in range(1):\n",
    "            response, cost = call_language_model(client, system_prompt, user_prompt, full_text, calculate_cost)\n",
    "            header_row_list.append(response)\n",
    "            data['cost'] += cost\n",
    "\n",
    "        header_counts = Counter(header_row_list)\n",
    "\n",
    "        most_common_count = header_counts.most_common(1)[0][1]  # Get the count of the most common string\n",
    "\n",
    "        # Find all strings that have this highest count\n",
    "        most_common_headers = [header for header, count in header_counts.items() if count == most_common_count]\n",
    "\n",
    "        # # Handle the situation based on the number of most common strings\n",
    "        if len(most_common_headers) == 1:\n",
    "            # Only one most common string\n",
    "            most_common_header = most_common_headers[0]\n",
    "            data['header_row'] = most_common_header\n",
    "        elif len(most_common_headers) > 1:\n",
    "            # Multiple strings with the same highest frequency\n",
    "            data.update({\n",
    "                \"error\": \"Multiple common headers\",\n",
    "                \"comment\": most_common_headers\n",
    "            })\n",
    "            log_to_wandb(data)\n",
    "            continue\n",
    "        else:\n",
    "            data.update({\n",
    "                \"error\": \"No header row found\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Regular expression pattern to check if string starts with '[' and ends with ']'\n",
    "        pattern = r'^\\[.*\\]$'\n",
    "\n",
    "        # Check if the response string matches the pattern\n",
    "        if re.match(pattern, most_common_header):\n",
    "            # Process the string if it matches\n",
    "            column_names = most_common_header # Adjust as needed\n",
    "\n",
    "            # check if response is 0. In that case, we check if the file might not be structured like a table, but more like a list (alternative document structure)\n",
    "            if column_names == \"[0]\":\n",
    "\n",
    "                user_prompt = config['prompts']['user_prompts']['alternative_doc_structure_prompt'].format(report=full_text)\n",
    "\n",
    "                response, cost = call_language_model(client, system_prompt, user_prompt, full_text, calculate_cost)\n",
    "                header_row_list.append(response)\n",
    "                data['cost'] += cost\n",
    "\n",
    "                # check response\n",
    "                if response == \"[1]\":\n",
    "\n",
    "                    alternative_document_structure = True\n",
    "\n",
    "                    # find the highest grundkapital percentage in the document\n",
    "                    user_prompt = config['prompts']['user_prompts']['alternative_doc_extraction_prompt'].format(report=full_text)\n",
    "\n",
    "                    response, cost = call_language_model(client, system_prompt, user_prompt, full_text, calculate_cost)\n",
    "                    header_row_list.append(response)\n",
    "                    data['cost'] += cost\n",
    "\n",
    "                elif response == \"[0]\":\n",
    "                    data.update({\n",
    "                        \"error\": \"Header row not found\",\n",
    "                    })\n",
    "                    log_to_wandb(data)\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    data.update({\n",
    "                        \"error\": \"Unexpected response from GPT model to the question about alternative document structure\",\n",
    "                        \"comment\": response,\n",
    "                    })\n",
    "                    log_to_wandb(data)\n",
    "                    continue\n",
    "\n",
    "        else:\n",
    "            data.update({\n",
    "                \"error\": \"Unexpected response from GPT model to the question about the header row\",\n",
    "                \"comment\": response,\n",
    "            })\n",
    "            log_to_wandb(data)\n",
    "            continue\n",
    "\n",
    "        if alternative_document_structure == False:\n",
    "            user_prompt = config['prompts']['user_prompts']['regular_extraction_prompt'].format(column_names=column_names, report=full_text)\n",
    "\n",
    "            response, cost = call_language_model(client, system_prompt, user_prompt, full_text, calculate_cost)\n",
    "            header_row_list.append(response)\n",
    "            data['cost'] += cost\n",
    "\n",
    "        try:\n",
    "            # Parse percentage list from response for the entire document\n",
    "            percentage_list = ast.literal_eval(response)\n",
    "            if percentage_list:\n",
    "                highest_percentage = max(percentage_list)\n",
    "                data['Presence'] = round(highest_percentage, 2)\n",
    "                data['all_percentage_values'] = percentage_list\n",
    "        except (SyntaxError, ValueError) as e:\n",
    "            data[\"comment\"] += f\"{response} \"\n",
    "            data.update({\n",
    "                \"error\": \"Error parsing list of percentages\"\n",
    "            })\n",
    "            log_to_wandb(data)\n",
    "            continue\n",
    "\n",
    "        # sanity checks\n",
    "\n",
    "        if len(highest_percentage_list) > 1:\n",
    "            second_highest_percentage = sorted(highest_percentage_list, reverse=True)[1]\n",
    "            if highest_percentage - second_highest_percentage > 10:\n",
    "                data[\"comment\"] += \"Der ermittelte Wert weicht um mehr als 10 Prozentpunkte vom zweithöchsten Wert ab. \"\n",
    "                data.update({\n",
    "                    \"error\": \"Significant difference from second highest value\",\n",
    "                    \"Presence\": highest_percentage\n",
    "                })\n",
    "                log_to_wandb(data)\n",
    "                continue\n",
    "\n",
    "        # add check for if the highest percentage is greater than 99, flag it as an error\n",
    "        if highest_percentage > 99:\n",
    "            data.update({\n",
    "                \"error\": \"Highest percentage greater than 99\",\n",
    "                \"Presence\": highest_percentage\n",
    "            })\n",
    "            log_to_wandb(data)\n",
    "            continue\n",
    "\n",
    "        # if data['Presence_enhanced'] == data['Presence_predicted']:\n",
    "        #     data['correct'] = True\n",
    "        # else:\n",
    "        #     # Check for another row with the same ID_Key_original and Year_original\n",
    "        #     same_id_year_rows = df[(df['ID_Key_original'] == row['ID_Key_original']) & \n",
    "        #                         (df['Year_original'] == row['Year_original'])]\n",
    "\n",
    "        #     # Check if any of those rows have Presence_enhanced equal to highest_percentage\n",
    "        #     if any(same_id_year_rows['Presence_enhanced'] == data['Presence_predicted']):\n",
    "        #         data['correct'] = True\n",
    "        #         data[\"comment\"] += \"Der ermittelte Wert stammt aus dem anderen Bericht diesen Jahres und ist dort korrekt ermittelt. \"\n",
    "        #     else:\n",
    "        #         data['correct'] = False\n",
    "\n",
    "        # Log the data\n",
    "        log_to_wandb(data)\n",
    "\n",
    "    # Save the table to W&B\n",
    "    wandb.log({\"results\": table})\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fast-silence-123</strong> at: <a href='https://wandb.ai/fuels/hv-praesenzen/runs/w5a6j7qv' target=\"_blank\">https://wandb.ai/fuels/hv-praesenzen/runs/w5a6j7qv</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231222_121422-w5a6j7qv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.log({\"results\": table})\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hv_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
